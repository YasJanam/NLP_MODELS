{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## **Trainer**\n",
        "\n",
        "خودش اتومات انجام میده Trainer کد زیر پیاده سازی دستی همون چیزی هست که کلاس\n",
        "\n",
        "(Trainer in HuggingFace یا PyTorch Lightning)\n",
        "    \n"
      ],
      "metadata": {
        "id": "zoEDGmhBIY1e"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JZLNGAr7Ei-z"
      },
      "outputs": [],
      "source": [
        "  for step in range(cfg.steps):\n",
        "    # lr\n",
        "    lr = lr_schedule(step)\n",
        "    for pg in optim.param_groups:\n",
        "      pg['lr'] = lr\n",
        "\n",
        "    x,y = get_batch('train')\n",
        "    with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n",
        "      logits, loss = model(x,y)\n",
        "      loss = loss / cfg.accum_steps\n",
        "    scaler.scale(loss).backward()\n",
        "\n",
        "    if (step + 1) % cfg.accum_steps == 0:\n",
        "      scaler.step(optim)\n",
        "      scaler.update()\n",
        "      optim.zero_grad(set_to_none = True)\n",
        "\n",
        "    running_loss += loss.item() * cfg.accum_steps\n",
        "\n",
        "    # Validation & logging every 100 steps\n",
        "    if(step + 1) % 100 == 0 or (step + 1) == cfg.steps:\n",
        "      model.eval()\n",
        "      with torch.no_grad():\n",
        "        vx,vy = get_batch('val')\n",
        "        _,vloss = model(vx,vy)\n",
        "      avg_train = running_loss / 100\n",
        "      running_loss = 0.0\n",
        "      print(f\"step {step+1}/{cfg.steps} | lr {lr:2e} | train_loss {avg_train:.4f} | val_loss {vloss.item():.4f}\")\n",
        "      model.train()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### خلاصه اعمال انجام شده در کد بالا :    "
      ],
      "metadata": {
        "id": "sqQLl50xPaBQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "  for step in range(cfg.steps):\n",
        "      1. lr تنظیم\n",
        "      2. batch گرفتن\n",
        "      3. forward + loss\n",
        "      4. backward (GradScaler با )\n",
        "      5. update optimizer (accum_steps هر)\n",
        "      6. validation & logging (step 100 هر)"
      ],
      "metadata": {
        "id": "BM5JEBrjiPgr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## تکنیک های مفید آموزش :      \n",
        "\n",
        "1. AMP (Automatic Mixed Precision)\n",
        "2. Scaler\n",
        "3. Gradient Accumulation\n",
        "\n",
        "\n",
        "فرآیند آموزش مدل های عمیق, به ویژه مدل های بزرگ مانند ترنسفورمرها,میتواند شامل تکنیک های پیشرفته ای باشد که برای بهینه سازی عملکرد,سرعت و کارایی استفاده میشوند.تکنیک های بالا از جمله این روش ها هستند"
      ],
      "metadata": {
        "id": "5xQ9_GrzTYOb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **AMP** **(Automatic Mixed Precision)**\n",
        "\n",
        "\n",
        "Mixed Precision -> دقت مختلط\n",
        "\n",
        "تسریع آموزش مدل و کاهش مصرف حافظه با استفاده از دقت مختلط"
      ],
      "metadata": {
        "id": "aTpw2XJ8W4Gh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "در آموزش معمولی مدل ها, محاسبات با دقت 32 بیتی انجام میشود.این دقت بالا باعث میشود محاسبات کندتر و مصرف حافظه بیشتر باشد\n",
        "\n",
        "\n",
        "از دقت 16 بیتی برای برخی محاسبات استفاده میکند,در حالی که محاسبات حساس همچنان با دقت 32 بیتی انجام میشودAMP\n",
        "\n",
        "مثال در پایتورچ :"
      ],
      "metadata": {
        "id": "i7tEPJWpXhpV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for torch.cuda.amp import autocast, GradScaler\n",
        "\n",
        "scaler = GradScaler()\n",
        "for data, target in dataloader:\n",
        "  optimizer.zero_grad()\n",
        "  with autocast():\n",
        "    output = model(data)\n",
        "    loss = criterion(output,target)\n",
        "  scaler.scale(loss).backward()\n",
        "  scaler.step(optimizer)\n",
        "  scaler.update()"
      ],
      "metadata": {
        "id": "PBmarFDrYgKM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "scaler ->  مدیریت مقیاس گرادیان ها در آموزش با دقت مختلط  "
      ],
      "metadata": {
        "id": "skovOy80aBck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Gradient Accumulation**\n",
        "\n",
        "آموزش مدل با بچ سایز های بزرگتر زمانی که حافظه محدود است\n",
        "\n",
        "گرادیان ها روی چندین بچ جمع میشوند و سپس بعد از تعداد مشخصی یچ, پارامتر ها به روز میشوند"
      ],
      "metadata": {
        "id": "rEu5SSVfamFa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "accumulation_steps = 4\n",
        "for i, (data, target) in enumerate(dataloader):\n",
        "  output = model(data)\n",
        "  loss = criterion(output, target)\n",
        "  loss = loss / accumulation_steps\n",
        "  loss.backward()\n",
        "  if (i + 1) % accumulation_steps == 0:\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()"
      ],
      "metadata": {
        "id": "HRb9s0Uyb9Bm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ▶ مقایسه روش قدیمی و تکنیک های بالا"
      ],
      "metadata": {
        "id": "g3wnrtwzfMdk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### دنیای قدیم (پروژه‌های ساده)"
      ],
      "metadata": {
        "id": "3E7il7-qfjh7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# آموزش پایه - همون چیزی که احتمالاً دیدی\n",
        "for epoch in range(epochs):\n",
        "    for batch in dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(batch)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()"
      ],
      "metadata": {
        "id": "3n0U-FE_febK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### دنیای جدید (پروژه‌های حرفه‌ای)"
      ],
      "metadata": {
        "id": "7oqQMm4dfne6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# آموزش پیشرفته - با همه تکنیک‌ها\n",
        "scaler = GradScaler()\n",
        "accumulation_steps = 4\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    for i, batch in enumerate(dataloader):\n",
        "        with autocast():\n",
        "            outputs = model(batch)\n",
        "            loss = criterion(outputs, labels) / accumulation_steps\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "        if (i + 1) % accumulation_steps == 0:\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            optimizer.zero_grad()"
      ],
      "metadata": {
        "id": "LEbejh3vfqq_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}